{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eab953a",
   "metadata": {},
   "source": [
    "# closed-world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  #closed-world\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "# Define the folder path\n",
    "folder_path = r'D:\\packetsizecw'\n",
    "\n",
    "def load_data(directory):\n",
    "    \"\"\"\n",
    "    Load data from ascii files\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    class_counter = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for fname in files:\n",
    "            cls, inst  = fname.split('-')\n",
    "            trace_class = int(cls)\n",
    "            file = open(os.path.join(root, fname), 'r')\n",
    "            print(file)\n",
    "            packetsize = []\n",
    "            for line in file:\n",
    "                pieces = line.strip(\"\\n\").split(':')\n",
    "                if int(pieces[1]) == 0:\n",
    "                    break\n",
    "                length = int(pieces[1])\n",
    "                packetsize.append(length)\n",
    "        \n",
    "            feature=extract(packetsize)\n",
    "            X.append(feature)\n",
    "            y.append(trace_class)\n",
    "\n",
    "    # wrap as numpy array\n",
    "    X, Y = np.array(X), np.array(y)\n",
    "    return X, Y\n",
    "\n",
    "# Define the feature extraction function\n",
    "def extract(sinste):  #sinste: list of packet sizes\n",
    "    \n",
    "    #first 4 features\n",
    "    features = []\n",
    "            \n",
    "    total = []\n",
    "    cum = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    inSize = 0\n",
    "    outSize = 0\n",
    "    inCount = 0\n",
    "    outCount = 0\n",
    "\n",
    "    for i in range(0, len(sinste)):\n",
    "        # incoming packets\n",
    "        if sinste[i]  > 0:\n",
    "            inSize += sinste[i] \n",
    "            inCount += 1\n",
    "            # cumulated packetsizes\n",
    "            if len(cum) == 0:\n",
    "                cum.append(sinste[i] )\n",
    "                total.append(sinste[i] )\n",
    "                pos.append(sinste[i] )\n",
    "                neg.append(0)\n",
    "            else:\n",
    "                cum.append(cum[-1] + sinste[i] )\n",
    "                total.append(total[-1] + abs(sinste[i] ))\n",
    "                pos.append(pos[-1] + sinste[i] )\n",
    "                neg.append(neg[-1] + 0)\n",
    "\n",
    "        # outgoing packets\n",
    "        if sinste[i]  < 0:\n",
    "            outSize += abs(sinste[i] )\n",
    "            outCount += 1\n",
    "            if len(cum) == 0:\n",
    "                cum.append(sinste[i] )\n",
    "                total.append(abs(sinste[i] ))\n",
    "                pos.append(0)\n",
    "                neg.append(abs(sinste[i] ))\n",
    "            else:\n",
    "                cum.append(cum[-1] + sinste[i] )\n",
    "                total.append(total[-1] + abs(sinste[i] ))\n",
    "                pos.append(pos[-1] + 0)\n",
    "                neg.append(neg[-1] + abs(sinste[i] ))\n",
    "\n",
    "    # add feature\n",
    "    features.append(inCount)\n",
    "    features.append(outCount)\n",
    "    features.append(outSize)\n",
    "    features.append(inSize)\n",
    "    cumFeatures = np.interp(np.linspace(total[0], total[-1], 101), total, cum)\n",
    "    for el in itertools.islice(cumFeatures, 1, None):\n",
    "        features.append(el)\n",
    "    return features\n",
    "X,y=load_data(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm  #cw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume X and y are loaded and preprocessed data\n",
    "\n",
    "# Scale the features to the range [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the scaled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(y_train)\n",
    "# y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Create an SVM classifier object\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {'C': [2048, 4096, 8192, 16384, 32768, 65536, 131072], 'gamma': [0.125, 0.25, 0.5, 1, 2, 4, 8]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, n_jobs=8, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "\n",
    "# Create a new SVM classifier with the best parameters\n",
    "svm_model = svm.SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'])\n",
    "\n",
    "# Train the SVM model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm  #ten-fold cross-validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assume X and y are loaded and preprocessed data\n",
    "\n",
    "# Scale the features to the range [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the scaled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# Create an SVM classifier object\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {'C': [2048, 4096, 8192, 16384, 32768, 65536, 131072], 'gamma': [0.125, 0.25, 0.5, 1, 2, 4, 8]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, n_jobs=8, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "\n",
    "# Create a new SVM classifier with the best parameters\n",
    "svm_model = svm.SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'])\n",
    "\n",
    "# Evaluate the SVM classifier using 10-fold cross-validation\n",
    "scores = cross_val_score(svm_model, X_train, y_train, cv=10, n_jobs=8)\n",
    "\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140645d3",
   "metadata": {},
   "source": [
    "# open-world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b465d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  #open-world\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "# Define the folder path\n",
    "mon_path=r'D:\\packetsizecw'\n",
    "unmon_path=r'D:\\packetsizeow'\n",
    "\n",
    "def load_mondata(directory):\n",
    "    \"\"\"\n",
    "    Load data from monitored websites\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    class_counter = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "#         if not unmon:\n",
    "#         files = [fname for fname in files if len(fname.split('-')) == 2]\n",
    "        for fname in files:\n",
    "            cls, inst  = fname.split('-')\n",
    "            trace_class = int(cls)\n",
    "            file = open(os.path.join(root, fname), 'r')\n",
    "            print(file)\n",
    "            packetsize = []\n",
    "            for line in file:\n",
    "                pieces = line.strip(\"\\n\").split(':')\n",
    "                if int(pieces[1]) == 0:\n",
    "                    break\n",
    "                length = int(pieces[1])\n",
    "                packetsize.append(length)\n",
    "                \n",
    "            feature=extract(packetsize)\n",
    "            X.append(feature)\n",
    "            y.append(trace_class)\n",
    "\n",
    "    # wrap as numpy array\n",
    "    X, Y = np.array(X), np.array(y)\n",
    "    return X, Y\n",
    "\n",
    "def load_unmondata(directory):\n",
    "    \"\"\"\n",
    "    Load data from unmonitored websites\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    class_counter = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "#         if not unmon:\n",
    "#         files = [fname for fname in files if len(fname.split('-')) == 2]\n",
    "        for fname in files:\n",
    "#             cls, inst  = fname.split('-')\n",
    "#             trace_class = int(cls)\n",
    "            file = open(os.path.join(root, fname), 'r')\n",
    "            print(file)\n",
    "            packetsize = []\n",
    "            for line in file:\n",
    "                pieces = line.strip(\"\\n\").split(':')\n",
    "                if int(pieces[1]) == 0:\n",
    "                    break\n",
    "                length = int(pieces[1])\n",
    "                packetsize.append(length)\n",
    "        \n",
    "            feature=extract(packetsize)\n",
    "            X.append(feature)\n",
    "            y=[100]*len(files)\n",
    "\n",
    "    # wrap as numpy array\n",
    "    X, Y = np.array(X), np.array(y)\n",
    "    return X, Y\n",
    "\n",
    "# Define the feature extraction function\n",
    "def extract(sinste): #sinste: list of packet sizes\n",
    "    features = []\n",
    "    #first 4 features    \n",
    "    total = []\n",
    "    cum = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    inSize = 0\n",
    "    outSize = 0\n",
    "    inCount = 0\n",
    "    outCount = 0\n",
    "\n",
    "    for i in range(0, len(sinste)):\n",
    "        # incoming packets\n",
    "        if sinste[i]  > 0:\n",
    "            inSize += sinste[i] \n",
    "            inCount += 1\n",
    "            # cumulated packetsizes\n",
    "            if len(cum) == 0:\n",
    "                cum.append(sinste[i] )\n",
    "                total.append(sinste[i] )\n",
    "                pos.append(sinste[i] )\n",
    "                neg.append(0)\n",
    "            else:\n",
    "                cum.append(cum[-1] + sinste[i] )\n",
    "                total.append(total[-1] + abs(sinste[i] ))\n",
    "                pos.append(pos[-1] + sinste[i] )\n",
    "                neg.append(neg[-1] + 0)\n",
    "\n",
    "        # outgoing packets\n",
    "        if sinste[i]  < 0:\n",
    "            outSize += abs(sinste[i] )\n",
    "            outCount += 1\n",
    "            if len(cum) == 0:\n",
    "                cum.append(sinste[i] )\n",
    "                total.append(abs(sinste[i] ))\n",
    "                pos.append(0)\n",
    "                neg.append(abs(sinste[i] ))\n",
    "            else:\n",
    "                cum.append(cum[-1] + sinste[i] )\n",
    "                total.append(total[-1] + abs(sinste[i] ))\n",
    "                pos.append(pos[-1] + 0)\n",
    "                neg.append(neg[-1] + abs(sinste[i] ))\n",
    "\n",
    "    # add feature\n",
    "    features.append(inCount)\n",
    "    features.append(outCount)\n",
    "    features.append(outSize)\n",
    "    features.append(inSize)\n",
    "    cumFeatures = np.interp(np.linspace(total[0], total[-1], 101), total, cum)\n",
    "    for el in itertools.islice(cumFeatures, 1, None):\n",
    "        features.append(el)\n",
    "    return features\n",
    "\n",
    "X_mon,y_mon=load_mondata(mon_path)\n",
    "X_unmon,y_unmon=load_unmondata(unmon_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34149d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=0.8\n",
    "X_mon_tr = X_mon[:int(X_mon.shape[0]*sr)]\n",
    "y_mon_tr = y_mon[:int(X_mon.shape[0]*sr)]\n",
    "X_unmon_tr = X_unmon[:1000]\n",
    "y_unmon_tr = y_unmon[:1000]\n",
    "\n",
    "X_mon_te = X_mon[int(X_mon.shape[0]*sr):]\n",
    "y_mon_te = y_mon[int(X_mon.shape[0]*sr):]\n",
    "X_unmon_te = X_unmon[1000:]\n",
    "y_unmon_te = y_unmon[1000:]\n",
    "\n",
    "X_train = np.concatenate([X_mon_tr, X_unmon_tr])\n",
    "y_train = np.concatenate([y_mon_tr, y_unmon_tr])\n",
    "X_test=np.concatenate([X_mon_te, X_unmon_te])\n",
    "y_test=np.concatenate([y_mon_te, y_unmon_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa17652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assume X and y are loaded and preprocessed data\n",
    "\n",
    "# Scale the features to the range [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the scaled data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(y_train)\n",
    "# y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Create an SVM classifier object\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {'C': [2048, 4096, 8192, 16384, 32768, 65536, 131072], 'gamma': [0.125, 0.25, 0.5, 1, 2, 4, 8]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, n_jobs=8, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "\n",
    "# Create a new SVM classifier with the best parameters\n",
    "svm_model = svm.SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'])\n",
    "\n",
    "# Train the SVM model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, precision, and recall of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate TPR and FPR for each class\n",
    "num_classes = len(label_encoder.classes_)\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "for i in range(num_classes):\n",
    "    tp = cm[i,i]\n",
    "    fp = np.sum(cm[:,i]) - tp\n",
    "    fn = np.sum(cm[i,:]) - tp\n",
    "    tn = np.sum(cm) - tp - fp - fn\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    \n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    \n",
    "    print(f\"Class {i} - TPR: {tpr:.4f}, FPR: {fpr:.4f}\")\n",
    "\n",
    "# Calculate macro-average TPR and FPR\n",
    "macro_tpr = np.mean(tpr_list)\n",
    "macro_fpr = np.mean(fpr_list)\n",
    "print(f\"Macro-average TPR: {macro_tpr:.4f}, FPR: {macro_fpr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
